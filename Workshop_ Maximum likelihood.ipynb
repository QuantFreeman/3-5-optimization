{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import statsmodels.api as sm \n",
    "import numpy as np \n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bisection\n",
    "\n",
    "\n",
    "One of the most common algorithms for numerical root-finding is *bisection*.\n",
    "\n",
    "To understand the idea, recall the well-known game where:\n",
    "\n",
    "- Player A thinks of a secret number between 1 and 100  \n",
    "- Player B asks if it’s less than 50  \n",
    "  \n",
    "  - If yes, B asks if it’s less than 25  \n",
    "  - If no, B asks if it’s less than 75  \n",
    "  \n",
    "\n",
    "And so on.\n",
    "\n",
    "This is bisection, a relative of [binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm). It works for all sufficiently well behaved increasing continuous functions with $ f(a) < 0 < f(b) $. \n",
    "\n",
    "Write an implementation of the bisection algorith, `bisect(f, lower, upper, tol)` which, given a function `f`, a lower bound `lower` and an upper bound `upper` finds the point `x` where `f(x) = 0`. The parameter `tol` is a numerical tolerance, you should stop once your step size is smaller than `tol`.\n",
    "\n",
    "\n",
    "Use it to minimize the function:\n",
    "\n",
    "$$\n",
    "f(x) = \\sin(4 (x - 1/4)) + x + x^{20} - 1 \\tag{2}\n",
    "$$\n",
    "\n",
    "in python: `lambda x: np.sin(4 * (x - 1/4)) + x + x**20 - 1`\n",
    "\n",
    "The value where f(x) = 0 should be around `0.408`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40826416015625"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bisect(f, lower, upper, tol):\n",
    "    while upper - lower > tol:\n",
    "        yup = f(upper)\n",
    "        ylow = f(lower)\n",
    "        if yup * ylow < 0:\n",
    "            xmid = (upper + lower) / 2\n",
    "            ymid = f(xmid)\n",
    "            if ymid * yup < 0:\n",
    "                lower = xmid\n",
    "            else:\n",
    "                upper = xmid\n",
    "        else:\n",
    "            return 'didnt work (type 1)'\n",
    "    if xmid:\n",
    "        return xmid\n",
    "    else:\n",
    "        return 'didnt work (type 2)'\n",
    "    \n",
    "f = lambda x: np.sin(4 * (x - 1/4)) + x + x**20 - 1\n",
    "bisect(f, -1, +1, 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 (stretch) Recursive Bisect\n",
    "\n",
    "Write a recursive version of the bisection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40826416015625"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# while difference in bounds > tol:\n",
    "# check for sign\n",
    "def r_bisect(f, lower, upper, tol, xmid='not set'):\n",
    "    yup = f(upper)\n",
    "    ylow = f(lower)\n",
    "    while upper - lower > tol:\n",
    "        if yup * ylow < 0:\n",
    "            xmid = (upper + lower) / 2\n",
    "            ymid = f(xmid)\n",
    "\n",
    "            if ymid * yup < 0:\n",
    "                return r_bisect(f, xmid, upper, tol, xmid)\n",
    "            else:\n",
    "                return r_bisect(f, lower, xmid, tol, xmid)\n",
    "        else:\n",
    "            return 'didnt work, adjust bounds'\n",
    "    return xmid\n",
    "    \n",
    "f = lambda x: np.sin(4 * (x - 1/4)) + x + x**20 - 1\n",
    "r_bisect(f, -1, +1, 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Movies Regression\n",
    "\n",
    "Write the best linear regression model you can on the [Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset?select=ratings.csv) to predict the profitability of a movie (revenue - budget). Maintain the interpretability of the model.\n",
    "\n",
    "Few notes:\n",
    "\n",
    "1. Clean your data! Movies where the budget or revenue are invalid should be thrown out\n",
    "\n",
    "2. Be creative with feature engineering. You can include processing to one-hot encode the type of movie, etc.\n",
    "\n",
    "3. The model should be useful for someone **who is thinking about making a movie**. So features like the popularity can't be used. You could, however, use the ratings to figure out if making \"good\" or \"oscar bait\" movies is a profitable strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPi9xcx3mywk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sasha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "def director_finder_3000(series):\n",
    "    for e in eval(series):\n",
    "        if e['job'] == 'Director':\n",
    "            return e['name']\n",
    "        \n",
    "data_url = r'../Data/movie_data/credits.csv'\n",
    "dataframe = pd.read_csv(data_url)\n",
    "cred = dataframe\n",
    "cred['director'] = cred['crew'].apply(director_finder_3000)\n",
    "\n",
    "data_url = r'../Data/movie_data/movies_metadata.csv'\n",
    "dataframe = pd.read_csv(data_url)\n",
    "meta = dataframe\n",
    "\n",
    "meta['id'] = pd.to_numeric(meta['id'], errors='coerce')\n",
    "meta = meta.fillna(0)\n",
    "cred['id'] = cred['id'].astype(int)\n",
    "meta['id'] = meta['id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sasha\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:1832: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 6, but rank is 3\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>profit</td>      <th>  R-squared:         </th>  <td>   0.636</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.636</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   261.0</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 31 Jan 2021</td> <th>  Prob (F-statistic):</th>  <td>5.78e-168</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:12:25</td>     <th>  Log-Likelihood:    </th> <td>-8.5081e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 45542</td>      <th>  AIC:               </th>  <td>1.702e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 45538</td>      <th>  BIC:               </th>  <td>1.702e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC2</td>       <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                 <td> -132.6765</td> <td>   17.677</td> <td>   -7.505</td> <td> 0.000</td> <td> -167.323</td> <td>  -98.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>belongs_to_collection</th> <td>   -4.9608</td> <td>    1.432</td> <td>   -3.464</td> <td> 0.001</td> <td>   -7.768</td> <td>   -2.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>budget</th>                <td>   -0.2423</td> <td>    0.123</td> <td>   -1.973</td> <td> 0.049</td> <td>   -0.483</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>runtime</th>               <td>-1.454e+04</td> <td> 1935.134</td> <td>   -7.515</td> <td> 0.000</td> <td>-1.83e+04</td> <td>-1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vote_average</th>          <td> -719.3151</td> <td>   99.689</td> <td>   -7.216</td> <td> 0.000</td> <td> -914.702</td> <td> -523.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vote_count</th>            <td> 6.966e+04</td> <td> 3561.718</td> <td>   19.559</td> <td> 0.000</td> <td> 6.27e+04</td> <td> 7.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>budget2</th>               <td> 5.689e-09</td> <td> 1.04e-09</td> <td>    5.451</td> <td> 0.000</td> <td> 3.64e-09</td> <td> 7.73e-09</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>62705.528</td> <th>  Durbin-Watson:     </th>   <td>   1.971</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>126100899.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 7.223</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>260.381</td>  <th>  Cond. No.          </th>   <td>1.00e+16</td>   \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC2)<br/>[2] The condition number is large,  1e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 profit   R-squared:                       0.636\n",
       "Model:                            OLS   Adj. R-squared:                  0.636\n",
       "Method:                 Least Squares   F-statistic:                     261.0\n",
       "Date:                Sun, 31 Jan 2021   Prob (F-statistic):          5.78e-168\n",
       "Time:                        13:12:25   Log-Likelihood:            -8.5081e+05\n",
       "No. Observations:               45542   AIC:                         1.702e+06\n",
       "Df Residuals:                   45538   BIC:                         1.702e+06\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:                  HC2                                         \n",
       "=========================================================================================\n",
       "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------\n",
       "const                  -132.6765     17.677     -7.505      0.000    -167.323     -98.030\n",
       "belongs_to_collection    -4.9608      1.432     -3.464      0.001      -7.768      -2.154\n",
       "budget                   -0.2423      0.123     -1.973      0.049      -0.483      -0.002\n",
       "runtime               -1.454e+04   1935.134     -7.515      0.000   -1.83e+04   -1.07e+04\n",
       "vote_average           -719.3151     99.689     -7.216      0.000    -914.702    -523.928\n",
       "vote_count             6.966e+04   3561.718     19.559      0.000    6.27e+04    7.66e+04\n",
       "budget2                5.689e-09   1.04e-09      5.451      0.000    3.64e-09    7.73e-09\n",
       "==============================================================================\n",
       "Omnibus:                    62705.528   Durbin-Watson:                   1.971\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        126100899.610\n",
       "Skew:                           7.223   Prob(JB):                         0.00\n",
       "Kurtosis:                     260.381   Cond. No.                     1.00e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC2)\n",
       "[2] The condition number is large,  1e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = meta.merge(cred, on='id', how='left')\n",
    "df = df[['director', 'belongs_to_collection', 'budget', 'original_language', 'release_date', 'revenue', 'runtime',\n",
    "         'vote_average', 'vote_count']]\n",
    "# quantitative features\n",
    "money = ['budget','revenue']\n",
    "quant = ['vote_average', 'vote_count', 'revenue','budget','runtime']# 'release_month']\n",
    "# features to make into polynomials\n",
    "to_poly = ['budget']#,'runtime']\n",
    "# qualitative features\n",
    "qual = ['director','original_language']\n",
    "# externally sourced list of 'top 10' directors\n",
    "top_dir = ['Christopher Nolan','Steven Spielberg', 'Quentin Tarantino', 'Martin Scorsese', 'David Fincher','Stanley Kubrick','Robert Zemeckis','Ridley Scott','Francis Ford Coppola','Clint Eastwood']\n",
    "\n",
    "to_drop = ['original_language','release_date','director','revenue','profit']#'vote_average']\n",
    "\n",
    "\n",
    "# df['top10dir'] = df['director'].isin(top_dir).astype(int)\n",
    "\n",
    "    \n",
    "    \n",
    "# df['in_eng'] = df['original_language'] == 'en'\n",
    "# df['in_eng'] = df.in_eng.astype(int)\n",
    "\n",
    "df['belongs_to_collection'] = (df['belongs_to_collection'] != 0).astype(int)\n",
    "\n",
    "# df['release_month'] = (df['release_date'].astype(str)\n",
    "#                                          .str.slice(start=5,stop=7))\n",
    "# df['release_month'].value_counts()\n",
    "\n",
    "# force numeric columns into int, filling NAs with 0\n",
    "for col in quant:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "#     df[col+'_missing'] = (df[col] == 0).astype(int)\n",
    "\n",
    "\n",
    "for col in to_poly:\n",
    "    df[col + '2'] = df[col]**2\n",
    "    \n",
    "df['profit'] = (df['revenue'] - df['budget'])\n",
    "\n",
    "X = df.copy()\n",
    "X = X.drop(to_drop, 1)\n",
    "\n",
    "Y = df.profit\n",
    "for c in X.columns:\n",
    "    if X[c].std()<.001:\n",
    "        X = X.drop(c,1)\n",
    "        print(c)\n",
    "X = sm.add_constant(X)\n",
    "mod = sm.OLS(Y, X).fit(cov_type = 'HC2')\n",
    "mod.summary()\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Movies Manual Regression\n",
    "\n",
    "Use your `X` and `y` matrix from 2.1 to calculate the linear regression yourself using the normal equation $(X^T X)^{-1}X^Ty$.\n",
    "\n",
    "Verify that the coefficients are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -4.133007e+05\n",
       "1    1.020861e+07\n",
       "2   -2.608059e-01\n",
       "3   -4.021553e+03\n",
       "4   -2.737260e+05\n",
       "5    6.884777e+04\n",
       "6    5.717372e-09\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (X-transpose matrix times x-regular)inverted, times x-transpose, times y matrix\n",
    "realx = np.linalg.inv(X.T@X) @ X.T @ Y\n",
    "realx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Movies gradient descent regression\n",
    "\n",
    "Use your `X` and `y` matrix from 2.1 to calculate the linear regression yourself using **gradient descent**. \n",
    "\n",
    "Hint: use `scipy.optimize` and remember we're finding the $\\beta$ that minimizes the squared loss function of linear regression: $f(\\beta) = (\\beta X - y)^2$. This will look like part 3 of this lecture.\n",
    "\n",
    "Verify your coefficients are similar to the ones in 2.1 and 2.2. They won't necessarily be exactly the same, but should be roughly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = lambda betas : np.sum((Y - (X @ betas)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   direc: array([[-3.54267958e+13,  1.72424548e+12,  1.02236926e+05,\n",
      "         2.93130097e+11,  1.31557898e+11,  9.58918197e+09,\n",
      "        -2.23884400e-03],\n",
      "       [-1.46925106e+12, -1.82160443e+12,  7.79112518e+03,\n",
      "        -3.93748106e+09,  3.38942767e+11, -1.99523947e+08,\n",
      "        -4.97649789e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.00000000e+00],\n",
      "       [-4.27037748e+11,  4.26639628e+11,  3.22542387e+03,\n",
      "         1.12074534e+09,  5.92247369e+10, -2.77263209e+08,\n",
      "         1.28499384e-05],\n",
      "       [ 1.13380152e+13, -1.14593997e+13,  2.12179043e+06,\n",
      "         3.04903956e+10, -4.29470297e+12, -6.32103006e+10,\n",
      "        -5.48987933e-03],\n",
      "       [ 2.17634430e+14,  7.80692666e+00,  5.25615173e+07,\n",
      "        -2.92343956e+12,  9.42285605e+12,  2.44698443e+11,\n",
      "        -3.32973878e-01],\n",
      "       [-1.23879905e+09, -2.10988067e+09,  5.33107691e+00,\n",
      "         1.88205978e+07,  4.21446381e+07, -2.89014314e+05,\n",
      "        -7.03265189e-08]])\n",
      "     fun: 4.453848586703936e+19\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 1776\n",
      "     nit: 20\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-4.09689366e+05,  1.02094115e+07, -2.60815215e-01, -4.02544235e+03,\n",
      "       -2.74146212e+05,  6.88470364e+04,  5.71775014e-09])\n"
     ]
    }
   ],
   "source": [
    "betas = np.random.rand(X.shape[1])\n",
    "est = minimize(target, betas, method='powell', options= {'maxiter':15000, 'ftol':0.000001} )\n",
    "print(est)\n",
    "estx = est.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -3.611317e+03\n",
       "1   -8.024525e+02\n",
       "2    9.299447e-06\n",
       "3    3.889105e+00\n",
       "4    4.202073e+02\n",
       "5    7.336304e-01\n",
       "6   -3.777937e-13\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realx - estx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPIPzivigAhB3FeR6Q96N8T",
   "collapsed_sections": [],
   "name": "Workshop: Maximum likelihood.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
